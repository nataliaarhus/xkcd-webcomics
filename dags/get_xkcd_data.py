"""
get_data
DAG auto-generated by Astro Cloud IDE.
"""

from airflow.decorators import dag
from airflow.models import Variable
from astro import sql as aql
import pandas as pd
from pendulum import datetime

from airflow.models import Variable
import psycopg2
from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError
import os
from dotenv import load_dotenv
import requests
import pandas as pd
from datetime import datetime

@dag(
    default_args={"owner": "Natalia", "retries": 3},
    schedule="0 0 * * *",
    start_date=datetime(2024, 1, 1),
    catchup=False
)

@aql.dataframe(task_id="Get_xkcd_data")
def Get_xkcd_data_func():
    def get_credentials():
        load_dotenv()
        credentials = {'host': os.environ['host'], 'database': os.environ['database'], 'user': os.environ['user'],
                       'password': os.environ['password'], 'port': os.environ['port']}
        return credentials

    def connect_db():
        credentials = get_credentials()
        try:
            conn = psycopg2.connect(**credentials)
            return conn
        except:
            raise

    def load_to_db(table_name, schema_name, df):
        try:
            credentials = get_credentials()
            host = credentials['host']
            database = credentials['database']
            user = credentials['user']
            password = credentials['password']
            port = credentials['port']
            params = f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}'
            # print("params: " + params)
            engine = create_engine(params, isolation_level='AUTOCOMMIT')
            print("############### Loading to table : ", table_name)
            df.to_sql(name=table_name, con=engine, schema=schema_name, if_exists='append', index=False, method='multi')
        except SQLAlchemyError as e:
            error_message = f"Error loading data to table {schema_name}.{table_name}: {str(e)}"
            raise RuntimeError(error_message)

    def get_data_from_xkcd(num):
        url = 'https://xkcd.com/' + str(num) + '/info.0.json'
        header = {'Content-Type': 'application/json'}

        try:
            api_call = requests.get(url=url, headers=header)
            api_call.raise_for_status()
            result = api_call.json()
            df = pd.DataFrame([result])
            return df

        except requests.exceptions.RequestException as e:
            print("Error fetching data:", e)

        except Exception as e:
            print("An error occurred:", e)

    def find_max_value():
        url = 'https://xkcd.com/info.0.json'
        header = {'Content-Type': 'application/json'}
        api_call = requests.get(url=url, headers=header)
        max_value = api_call.json()['num']
        return max_value

    def find_last_value():
        conn = connect_db()
        sql_query = "select max(max_value) from etl.load_status where table_name='xkcd_records'"
        cursor = conn.cursor()
        cursor.execute(sql_query)
        result = cursor.fetchone()
        if result[0] is not None:
            last_value = result[0]
        else:
            last_value = 0
        cursor.close()
        conn.close()
        return last_value

    def write_etl_status(df, table_name):
        rows = df.shape[0]
        min_value = df['num'].min()
        max_value = df['num'].max()
        executed_at = datetime.utcnow()
        df = pd.DataFrame([{
            'table_name': table_name,
            'count_records': rows,
            'min_value': min_value,
            'max_value': max_value,
            'executed_at': executed_at
        }])
        print(df)
        load_to_db('load_status', 'etl', df)

    def validate_df(df):
        expected_columns = ['month', 'num', 'link', 'year', 'news', 'safe_title', 'transcript', 'alt', 'img', 'title',
                            'day']
        passed_columns = df.columns.tolist()

        if expected_columns == passed_columns:
            # print('Check passed')
            return df
        else:
            print('Columns dont match')
            if len(passed_columns) > len(expected_columns):
                columns_to_drop = [col for col in passed_columns if col not in expected_columns]
                df = df.drop(columns=columns_to_drop)
                print('Dropped extra columns')
                return df
            else:
                print('Fewer columns passed, continue')
                return df

    def main():
        last_value = find_last_value()
        max_value = find_max_value()

        # Check if there are any new records
        # If yes, extract them, validate them, and append into a df
        if max_value > last_value:
            df = pd.DataFrame()
            for i in range(last_value + 1, max_value + 1):
                print(i)
                df1 = get_data_from_xkcd(i)
                if df1 is not None:
                    df1 = validate_df(df1)
                    df = pd.concat([df, df1])
                else:
                    print('Entry does not exist:', i)
                    continue

            # Create cost and creation date columns
            df = df.assign(
                cost_eur=df['title'].apply(lambda x: len(str(x.replace(' ', '')))) * 5,
                created_at=pd.to_datetime(
                    df['year'].astype(str) + '-' + df['month'].astype(str) + '-' + df['day'].astype(str))

            )
            df.drop(['month', 'year', 'day'], inplace=True, axis=1)

            # Load df to the db and update etl load status
            table_name = 'xkcd_records'
            schema_name = 'public'
            load_to_db(table_name, schema_name, df)
            write_etl_status(df, table_name)

        else:
            print('No new entries')

    main()


default_args = {
    "owner": "natalia_gumowska@hotmail.com,Open in Cloud IDE",
}


def get_data():
    Get_xkcd_data = Get_xkcd_data_func()


dag_obj = get_data()
